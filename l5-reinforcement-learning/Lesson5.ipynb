{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db68f9ed-b3d6-4722-92da-d23681ceaab2",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to this Excercise. We are now going to use our new skills to build our first Deep Learning Reinforcement Learning Model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1308720-0eb4-44dc-8091-06d3b53143eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 completed\n",
      "Episode 200 completed\n",
      "Episode 300 completed\n",
      "Episode 400 completed\n",
      "Episode 500 completed\n",
      "Episode 600 completed\n",
      "Episode 700 completed\n",
      "Episode 800 completed\n",
      "Episode 900 completed\n",
      "Episode 1000 completed\n",
      ".A...\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "....G\n",
      "\n",
      "..A..\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "....G\n",
      "\n",
      ".....\n",
      "..A..\n",
      ".....\n",
      ".....\n",
      "....G\n",
      "\n",
      ".....\n",
      ".....\n",
      "..A..\n",
      ".....\n",
      "....G\n",
      "\n",
      ".....\n",
      ".....\n",
      "...A.\n",
      ".....\n",
      "....G\n",
      "\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "...A.\n",
      "....G\n",
      "\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "....A\n",
      "....G\n",
      "\n",
      ".....\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "....A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium as gym\n",
    "#from gymnasium import spaces\n",
    "#from gymnasium import Env\n",
    "\n",
    "class SimpleGridEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, grid_size=5):\n",
    "        super(SimpleGridEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = gym.spaces.Discrete(4)  # 4 actions: up, down, left, right\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([grid_size, grid_size])\n",
    "        self.state = None\n",
    "        self.goal = (grid_size - 1, grid_size - 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return np.array(self.state, dtype=np.int32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        \n",
    "        if action == 0:  # up\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # down\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        elif action == 2:  # left\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # right\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        \n",
    "        self.state = (x, y)\n",
    "        \n",
    "        done = self.state == self.goal\n",
    "        reward = 1 if done else -0.1\n",
    "        \n",
    "        return np.array(self.state, dtype=np.int32), reward, done, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        grid[self.goal] = 'G'\n",
    "        x, y = self.state\n",
    "        grid[x, y] = 'A'\n",
    "        print(\"\\n\".join([\"\".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "def train_agent(env, agent, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = tuple(env.reset())\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = tuple(next_state)\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1} completed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = SimpleGridEnv()\n",
    "    agent = QLearningAgent(env)\n",
    "    train_agent(env, agent, episodes=1000)\n",
    "\n",
    "    state = tuple(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = tuple(state)\n",
    "        env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfec5c-0944-470e-a81e-f12aae7668ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.action_space = gym.spaces.Discrete(1)  # Action space (predict F_1_d_returns)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)  # State space (1_d_returns, 2_d_returns)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to initial state\n",
    "        self.current_step = 0\n",
    "        self.state = self.df.iloc[self.current_step, 1:3].values  # Start with first row's 1_d_returns and 2_d_returns\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take an action (not relevant here as we are predicting)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        if done:\n",
    "            next_state = self.state\n",
    "        else:\n",
    "            next_state = self.df.iloc[self.current_step, 1:3].values\n",
    "        reward = 0  # No reward for predicting\n",
    "        info = {}   # Additional information (if needed)\n",
    "        return next_state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8797-12b9-4b7d-b022-1fa60d69d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80a4d3-91a2-4eb4-8be3-953bc57218f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1780ed-c99f-4f9a-b8b3-dbcadc2a2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98475236-e671-4db2-99f9-995e0edd0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047b240-e06f-41d0-84de-0202f11bf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom model\n",
    "def build_model(input_shape, nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))  # Adjust input shape here\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004ad10-13f0-4953-8220-24d54f0d2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20d8d7-458c-4d59-8041-3a783f12017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60f436-be34-448e-913c-50de93a60095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e1261-1d6d-4c4d-bce9-38519dcc1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37572412-02b0-4b7b-a3d1-b174c7c3dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84c88a-887e-42bf-893b-148d62a011a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9446c-b865-4964-a784-5e52378afdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ffc392-08db-463c-a2a3-a534d92c362e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Creating a OutOfGraphReplayBuffer replay memory with the following parameters:\n",
      "INFO:absl:\t observation_shape: (1, 2)\n",
      "INFO:absl:\t observation_dtype: <class 'numpy.uint8'>\n",
      "INFO:absl:\t terminal_dtype: <class 'numpy.uint8'>\n",
      "INFO:absl:\t stack_size: 1\n",
      "INFO:absl:\t replay_capacity: 100000\n",
      "INFO:absl:\t batch_size: 32\n",
      "INFO:absl:\t update_horizon: 1\n",
      "INFO:absl:\t gamma: 0.990000\n",
      "INFO:absl:\t checkpoint_duration: 4\n",
      "INFO:absl:\t keep_every: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot sample a batch with fewer than stack size (1) + update_horizon (1) transitions.\n  In call to configurable 'WrappedReplayBuffer' (<class 'dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 61\u001b[0m\n\u001b[0;32m     55\u001b[0m sess \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Set up a replay buffer\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Set up a replay buffer with increased capacity\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m circular_replay_buffer\u001b[38;5;241m.\u001b[39mWrappedReplayBuffer(\n\u001b[0;32m     62\u001b[0m     observation_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m     63\u001b[0m     stack_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     64\u001b[0m     replay_capacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)  \u001b[38;5;66;03m# Increased capacity\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Create the agent\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Create the agent with decreased min replay history\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Create the agent with a higher min replay history\u001b[39;00m\n\u001b[0;32m     70\u001b[0m agent \u001b[38;5;241m=\u001b[39m dqn_agent\u001b[38;5;241m.\u001b[39mDQNAgent(\n\u001b[0;32m     71\u001b[0m     sess,\n\u001b[0;32m     72\u001b[0m     num_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     target_update_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     82\u001b[0m     epsilon_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gin\\config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m scope_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scope_str) \u001b[38;5;28;01mif\u001b[39;00m scope_str \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1604\u001b[0m err_str \u001b[38;5;241m=\u001b[39m err_str\u001b[38;5;241m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[1;32m-> 1605\u001b[0m utils\u001b[38;5;241m.\u001b[39maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gin\\utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m proxy \u001b[38;5;241m=\u001b[39m ExceptionProxy()\n\u001b[0;32m     40\u001b[0m ExceptionProxy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exception)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mwith_traceback(exception\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39mnew_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dopamine\\replay_memory\\circular_replay_buffer.py:918\u001b[0m, in \u001b[0;36mWrappedReplayBuffer.__init__\u001b[1;34m(self, observation_shape, stack_size, use_staging, replay_capacity, batch_size, update_horizon, gamma, wrapped_memory, max_sample_attempts, extra_storage_types, observation_dtype, terminal_dtype, action_shape, action_dtype, reward_shape, reward_dtype)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m OutOfGraphReplayBuffer(\n\u001b[0;32m    902\u001b[0m       observation_shape,\n\u001b[0;32m    903\u001b[0m       stack_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    915\u001b[0m       reward_dtype\u001b[38;5;241m=\u001b[39mreward_dtype,\n\u001b[0;32m    916\u001b[0m   )\n\u001b[1;32m--> 918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sampling_ops(use_staging)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dopamine\\replay_memory\\circular_replay_buffer.py:953\u001b[0m, in \u001b[0;36mWrappedReplayBuffer.create_sampling_ops\u001b[1;34m(self, use_staging)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cpu:*\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    952\u001b[0m   transition_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_transition_elements()\n\u001b[1;32m--> 953\u001b[0m   transition_tensors \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnumpy_function(\n\u001b[0;32m    954\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample_transition_batch,\n\u001b[0;32m    955\u001b[0m       [],\n\u001b[0;32m    956\u001b[0m       [return_entry\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m return_entry \u001b[38;5;129;01min\u001b[39;00m transition_type],\n\u001b[0;32m    957\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay_sample_py_func\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    958\u001b[0m   )\n\u001b[0;32m    959\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_transition_shape(transition_tensors, transition_type)\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;66;03m# Unpack sample transition into member variables.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dopamine\\replay_memory\\circular_replay_buffer.py:599\u001b[0m, in \u001b[0;36mOutOfGraphReplayBuffer.sample_transition_batch\u001b[1;34m(self, batch_size, indices)\u001b[0m\n\u001b[0;32m    597\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m   indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_index_batch(batch_size)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m batch_size\n\u001b[0;32m    602\u001b[0m transition_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transition_elements(batch_size)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dopamine\\replay_memory\\circular_replay_buffer.py:540\u001b[0m, in \u001b[0;36mOutOfGraphReplayBuffer.sample_index_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    538\u001b[0m   max_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_horizon\n\u001b[0;32m    539\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m max_id \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m min_id:\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot sample a batch with fewer than stack size \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) + update_horizon (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) transitions.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    543\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_horizon\n\u001b[0;32m    544\u001b[0m         )\n\u001b[0;32m    545\u001b[0m     )\n\u001b[0;32m    547\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    548\u001b[0m attempt_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot sample a batch with fewer than stack size (1) + update_horizon (1) transitions.\n  In call to configurable 'WrappedReplayBuffer' (<class 'dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer'>)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import dopamine\n",
    "import logging  # Add this line\n",
    "from dopamine.agents.dqn import dqn_agent\n",
    "from dopamine.replay_memory import circular_replay_buffer\n",
    "from dopamine.colab import utils as colab_utils\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Create your environment\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.action_space = gym.spaces.Discrete(1)  # Action space (predict F_1_d_returns)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)  # State space (1_d_returns, 2_d_returns)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to initial state\n",
    "        self.current_step = 0\n",
    "        self.state = self.df.iloc[self.current_step, 1:3].values  # Start with first row's 1_d_returns and 2_d_returns\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take an action (not relevant here as we are predicting)\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        if done:\n",
    "            next_state = self.state\n",
    "        else:\n",
    "            next_state = self.df.iloc[self.current_step, 1:3].values\n",
    "        reward = 0  # No reward for predicting\n",
    "        info = {}   # Additional information (if needed)\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# Create your environment\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Target_Returns': [-0.038076, 0.083333, 0.060577, -0.013599, -0.020221],\n",
    "    '1_d_returns': [-0.062030, -0.038076, 0.083333, 0.060577, -0.013599],\n",
    "    '2_d_returns': [-0.133681, -0.097744, 0.042084, 0.148958, 0.046154]\n",
    "})\n",
    "\n",
    "env = CustomEnv(df)\n",
    "\n",
    "# Set up logging\n",
    "LOG_PATH = '/tmp/dopamine/logs'\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a TensorFlow session\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "# Set up a replay buffer\n",
    "\n",
    "\n",
    "# Set up a replay buffer with increased capacity\n",
    "replay_buffer = circular_replay_buffer.WrappedReplayBuffer(\n",
    "    observation_shape=(1,) + env.observation_space.shape,\n",
    "    stack_size=1,\n",
    "    replay_capacity=100000)  # Increased capacity\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "# Create the agent with decreased min replay history\n",
    "# Create the agent with a higher min replay history\n",
    "agent = dqn_agent.DQNAgent(\n",
    "    sess,\n",
    "    num_actions=env.action_space.n,\n",
    "    observation_shape=(1,) + env.observation_space.shape,\n",
    "    observation_dtype=tf.float32,\n",
    "    stack_size=1,\n",
    "    network='dqn',\n",
    "    gamma=0.99,\n",
    "    update_horizon=1,\n",
    "    min_replay_history=1000,  # Increase min replay history\n",
    "    update_period=4,\n",
    "    target_update_period=100,\n",
    "    epsilon_fn=lambda x: 0.1)\n",
    "\n",
    "# Create a checkpoint directory\n",
    "checkpoint_dir = os.path.join(LOG_PATH, 'checkpoints')\n",
    "checkpoint_file_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Create a logger\n",
    "logger = colab_utils.Logger(LOG_PATH)\n",
    "\n",
    "# Initialize variables\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.begin_episode(obs)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        agent.end_episode(reward)\n",
    "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "\n",
    "        if len(replay_buffer) >= agent.min_replay_history:\n",
    "            experience = replay_buffer.sample(1)\n",
    "            agent.step(experience)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        logger.scalar_summary('Return', reward, step=episode)\n",
    "\n",
    "# Save the final checkpoint\n",
    "checkpoint_path = agent._saver.save(sess, checkpoint_file_prefix)\n",
    "print('Final checkpoint saved at: %s' % checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d84ea-6f0c-4925-aa93-519b37e13aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a262fe-3a92-41cb-affe-80c3cc766a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef5049-72f6-418e-b9fa-0ab1686772c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "input_shape = env.observation_space.shape[0]\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25263302-6ce2-40ea-b86a-cb16d70ecf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = build_model(input_shape, nb_actions)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486f0c5-a91a-42a4-b5d5-7416eefc0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the memory\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "# Define the policy\n",
    "policy = BoltzmannQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f445d7-e2c1-4bd8-a852-4adca71be1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DQN agent\n",
    "dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=nb_actions,\n",
    "               nb_steps_warmup=100, target_model_update=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b5c1c-bbf3-4278-bcd0-e442d3626b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Instantiate the optimizer\n",
    "optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daae944-1024-4675-8c5e-9fe1f9885b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442aeed-00c0-4a73-bfc6-6c1924d7b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "dqn.compile(optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d934662-33de-446b-9b36-db5a6bbcfea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "dqn.compile(optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1b10d-b112-480e-8147-52b7e081f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)\n",
    "\n",
    "# Predict using the trained agent\n",
    "obs = env.reset()  # Reset the environment\n",
    "for _ in range(len(df) - 1):\n",
    "    action = dqn.forward(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    # Here, obs contains the predicted F_1_d_returns for each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e20259-cc35-47e9-8af9-a472e46cb01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'C:\\\\Users\\\\cramk\\\\Documents\\\\Metin\\\\building-a-workflow-for-aI\\\\l5-reinforcement-learning\\\\–V': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python –V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87c55e8e-757c-43e7-96c2-357652d10c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03286f4e-89e8-4615-9ac2-52a0ded7874e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
